{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an introductory tutorial to the main features of plangym."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `reset` and `step` return the environment state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The main difference with the `gymnasium` API is that environment state is considered as important as observations, rewards and terminal flags. This is why plangym incorporates them to the tuples that the environment returns after calling `step` and `reset`:\n\n- The `reset` method will return a tuple of (state, observation, info) unless you pass `return_state=False` as an argument.\n\n- When `step` is called passing the environment state as an argument it will return a tuple containing `(state, obs, reward, terminated, truncated, info)`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import plangym\n\nenv = plangym.make(\"CartPole-v1\")\naction = env.action_space.sample()\n\nstate, obs, info = env.reset()\nstate, obs, reward, terminated, truncated, info = env.step(action, state)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "However, if you don't provide the environment state when calling `step`, the returned tuple will match the standard `gymnasium` interface:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "env = plangym.make(\"CartPole-v1\")\naction = env.action_space.sample()\n\nobs, info = env.reset(return_state=False)\nobs, reward, terminated, truncated, info = env.step(action)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing and modifying the environment state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get a copy of the environment's state calling `env.get_state()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03145539,  0.17749025,  0.01348916, -0.25611924])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = env.get_state()\n",
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And set the environment state using `env.set_state(state)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.set_state(state)\n",
    "assert (state == env.get_state()).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All plangym environments offer a `step_batch` method that allows vectorized steps of batches of states and actions. \n",
    "\n",
    "Calling `step_batch` with a list of states and actions will return a tuple of lists containing the step data for each of the states and actions provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "states = [state.copy() for _ in range(10)]\nactions = [env.action_space.sample() for _ in range(10)]\n\ndata = env.step_batch(states=states, actions=actions)\nnew_states, observs, rewards, terminateds, truncateds, infos = data\ntype(new_states), type(observs)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel step vectorization using multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing the argument `n_workers` to `plangym.make` will return an environment that steps a batch of actions and states in parallel using multiprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "env = plangym.make(\"CartPole-v1\", n_workers=2)\nstates = [state.copy() for _ in range(10)]\nactions = [env.action_space.sample() for _ in range(10)]\n\ndata = env.step_batch(states=states, actions=actions)\nnew_states, observs, rewards, terminateds, truncateds, infos = data\ntype(env), type(new_states), type(observs)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step vectorization using ray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to use ray actors to step the environment in parallel when calling `step_batch` by passing `ray=True` to `plangym.make`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import ray\nray.init()\n\nenv = plangym.make(\"CartPole-v1\", n_workers=2, ray=True)\nstates = [state.copy() for _ in range(10)]\nactions = [env.action_space.sample() for _ in range(10)]\n\ndata = env.step_batch(states=states, actions=actions)\nnew_states, observs, rewards, terminateds, truncateds, infos = data\ntype(env), type(new_states), type(observs)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}